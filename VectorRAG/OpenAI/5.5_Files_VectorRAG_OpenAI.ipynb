{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50d74f0-fdb7-43ee-ad33-37e774ebe845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (0.3.17)\n",
      "Requirement already satisfied: pandas in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (2.2.3)\n",
      "Collecting python-docx\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: pypdf in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (5.2.0)\n",
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (0.3.4)\n",
      "Collecting tiktoken\n",
      "  Using cached tiktoken-0.8.0-cp310-cp310-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (0.3.34)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (0.3.5)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from python-docx) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain-openai) (1.61.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\jyseo\\anaconda3\\envs\\graph\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.58.1->langchain-openai) (0.4.6)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached tiktoken-0.8.0-cp310-cp310-win_amd64.whl (884 kB)\n",
      "Installing collected packages: python-docx, openpyxl, tiktoken\n",
      "Successfully installed openpyxl-3.1.5 python-docx-1.1.2 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu langchain pandas python-docx pypdf openpyxl langchain-openai tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c05ac0b-0347-491c-b696-1d880160901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk\" #openai 키 입력\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature = 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b69340fd-2195-40de-87d5-ecf7bd190ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'd:/data/files/'\n",
    "all_texts = [] # 추출된 모든 텍스트 데이터를 저장할 저장소를 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54f5cf7-8d6e-4ae5-b039-a08e32dd1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from docx import Document\n",
    "\n",
    "# Word 파일(.docx)에서 텍스트 추출\n",
    "def load_docx(path):\n",
    "    doc = Document(path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip() != ''])\n",
    "# doc.paragraphs를 사용하여 문서 내 모든 문단(paragraphs)을 반복하면서 텍스트를 추출\n",
    "# strip() != ''을 사용하여 공백 문단 제거\n",
    "# \"\\n\".join(...)을 사용하여 각 문단을 줄바꿈(\\n)을 기준으로 하나의 문자열로 결합\n",
    "\n",
    "# Excel 파일(.xlsx)에서 텍스트 추출\n",
    "def load_excel(path):\n",
    "    xls = pd.ExcelFile(path)\n",
    "    all_sheets_texts = []\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = xls.parse(sheet_name)\n",
    "        all_sheets_texts.append(\"\\n\".join(map(str, df.values.ravel())))\n",
    "    return \"\\n\".join(all_sheets_texts)\n",
    "# xls.sheet_names: 파일 내 모든 시트 이름을 가져옴\n",
    "# xls.parse(sheet_name): 각 시트를 Pandas DataFrame으로 변환\n",
    "# df.values.ravel(): 모든 셀 값을 1차원 배열(numpy.ravel())로 변환\n",
    "# map(str, df.values.ravel()): 모든 셀 값을 문자열로 변환\n",
    "# \"\\n\".join(...): 시트 내용을 줄바꿈(\\n)을 기준으로 결합\n",
    "\n",
    "# PDF 파일에서 텍스트 추출\n",
    "def load_pdf(path):\n",
    "    try:\n",
    "        doc = PyPDFLoader(path).load()\n",
    "        return \"\\n\".join([page.page_content for page in doc])\n",
    "    except Exception as e: # PDF 파일이 손상되었거나 읽을 수 없을 경우 오류 메시지를 출력\n",
    "        print(f\"Error processing PDF {path}. Error: {e}\")\n",
    "        return \"\"\n",
    "# [page.page_content for page in doc]: 모든 페이지에서 텍스트(page_content) 추출\n",
    "# \"\\n\".join(...): 각 페이지의 텍스트를 줄바꿈(\\n)을 기준으로 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e67eac7-81c7-48e0-b100-e2835a357332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path 폴더 내 모든 파일과 하위 폴더를 탐색\n",
    "for subdir, dirs, files in os.walk(base_path): \n",
    "    # 파일 확장자 확인 및 적절한 함수 호출\n",
    "    for file in files:\n",
    "        filename = os.path.join(subdir, file) # 파일의 전체 경로 생성 (예, d:/data/files/report.pdf)\n",
    "        extension = os.path.splitext(filename)[1].lower() # 파일 확장자 추출 및 소문자로 변환\n",
    "\n",
    "        # 파일 타입에 따라 적절한 함수 호출\n",
    "        text_content = \"\"\n",
    "        try:\n",
    "            if extension == \".pdf\":\n",
    "                text_content = load_pdf(filename)\n",
    "            elif extension == \".docx\":\n",
    "                text_content = load_docx(filename)\n",
    "            elif extension in [\".xls\", \".xlsx\"]:\n",
    "                text_content = load_excel(filename)\n",
    "\n",
    "        # 예외 처리\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}. Error: {e}\")\n",
    "\n",
    "        # 텍스트가 정상적으로 추출되었을 경우(if text_content:), all_texts에 추가\n",
    "        if text_content:\n",
    "            all_texts.append(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a888b1e6-3349-4227-90ee-6c81cf21951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 모든 문서를 하나의 문자열로 합침\n",
    "combined_texts = \"\\n\".join(all_texts)\n",
    "\n",
    "# RecursiveCharacterTextSplitter를 사용하여 텍스트 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "texts = text_splitter.split_text(combined_texts) # 하나의 긴 문서를 여러 개의 청크로 분할\n",
    "\n",
    "# OpenAI 임베딩 모델 로드\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 변환된 벡터를 FAISS 벡터 데이터베이스에 저장\n",
    "vectorstore = FAISS.from_texts(texts, embeddings) \n",
    "vectorstore.save_local('d:/data/db_faiss_combined') # 추후 재사용을 위해 FAISS 벡터 데이터를 로컬 디렉토리에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4aef452-ca5c-41aa-a072-f20b587e655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 데이터베이스 로드 (FAISS.load_local)\n",
    "retriever = FAISS.load_local(\n",
    "    \"d:/data/db_faiss_combined\",  # 저장된 FAISS 데이터베이스 경로\n",
    "    OpenAIEmbeddings(),  \n",
    "    allow_dangerous_deserialization=True  \n",
    ").as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 8}\n",
    ") # as_retriever()를 사용하여 문서 검색 가능하도록 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18265979-3814-4099-91dc-a7c3c5ee4256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '마음 챙김의 리뷰는?', 'result': '마음 챙김에 대한 리뷰는 다음과 같습니다:\\n\\n1. \"페이지를 넘길 수밖에 없었어요!\" - 평점 1.9\\n2. \"생각보다 별로였어요.\" - 평점 1.2'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# RetrievalQA.from_chain_type()을 사용하여 QA 체인 구성\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=retriever, # FAISS 데이터베이스에서 유사한 문서를 검색\n",
    ")\n",
    "\n",
    "# 엑셀 기반의 질문\n",
    "query = \"마음 챙김의 리뷰는?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6bd136a-bf06-4c8f-a508-513d97e7c84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': '집중력 향상을 위해 좋은 활동은?', 'result': '집중력 향상을 위해 좋은 활동으로는 퍼즐과 보드게임이 있습니다. 퍼즐이나 보드게임은 논리적 사고와 집중력을 필요로 하므로 두뇌 운동에 매우 효과적입니다. 크로스워드 퍼즐, 스도쿠 풀기, 체스, 바둑, 장기 같은 전략적인 보드게임을 추천합니다.'}\n"
     ]
    }
   ],
   "source": [
    "# 워드 기반의 질문\n",
    "query = \"집중력 향상을 위해 좋은 활동은?\"\n",
    "response = qa_chain.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce6c88-c546-472f-9eb0-7b62d9bb7a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
